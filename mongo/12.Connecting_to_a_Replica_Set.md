# Chapter 12 Connecting to a Replica Set from Your Application

## Client-to-Replica Set Connection Behavior

- **Transparent to client**

    For replica sets, by default, drivers will connect to the primary and route all traffic to it. **The replica set quietly keeps hot standbys ready** in the background.

    All MongoDB drivers adhere to the **server discovery and monitoring (SDAM) spec**. They persistently monitor the topology of your replica set to detect any changes in your application’s ability to reach all members of the set.

- **Automatic failover**

    If a primary goes down, the driver will **automatically find the new primary** (once one is elected) and will route requests to it as soon as possible.

- **Automatic retry by option**

    With **retryable writes** turned on, the server maintains a **unique identifier for each write operation** and can therefore determine when the driver is attempting to retry a command that already succeeded.

## Waiting for Replication on Writes

using `writeConcern` to make sure **all writes are replicated to a majority** of the replica set and no rollback ever occurs

```
try {
   db.products.insertOne(
       { "_id": 10, "item": "envelopes", "qty": 100, type: "Self-Sealing" },
       { writeConcern: { "w" : "majority", "wtimeout" : 100 } }
   );
} catch (e) {
   print (e);
}

{ "acknowledged" : true, "insertedId" : 10 }

WriteConcernError({
   "code" : 64,
   "errInfo" : {
      "wtimeout" : true
   },
   "errmsg" : "waiting for replication timed out"
})”
```

## Custom Replication Guarantees

1. **tag members** by assigning them key/value pairs
2. **create a rule based on the classification**, `"name": {"key", number}`， number means how many groups are required
3. setup the rules in `getLastErrorModes`
4. specify the rule in `writeConcern`

### Guaranteeing One Server per Data Center

- `eachDC` is the name of the rule
- `dc` is the name of the tag that should be used in `getLastErrorModes`
- `2` is the **number of groups** that are needed to fulfill the rule, here means at least one server from `us-east` group and one server from `us-west` group
- pass the `eachDC` to the `writeConcern`

```
> var config = rs.config()
> config.members[0].tags = {"dc" : "us-east"}
> config.members[1].tags = {"dc" : "us-east"}
> config.members[2].tags = {"dc" : "us-east"}
> config.members[3].tags = {"dc" : "us-east"}
> config.members[4].tags = {"dc" : "us-west"}
> config.members[5].tags = {"dc" : "us-west"}
> config.members[6].tags = {"dc" : "us-west"}

> config.settings = {}
> config.settings.getLastErrorModes = [{"eachDC" : {"dc" : 2}}]
> rs.reconfig(config)

db.products.insertOne(
    { "_id": 10, "item": "envelopes", "qty": 100, type: "Self-Sealing" },
    { writeConcern: { "w" : "eachDC", wtimeout : 1000 } }
);
```

### Guaranteeing a Majority of Nonhidden Members

- nonhidden members `host0-3` are given a tag
- the hidden member `host4` is not given a tag

```
> var config = rs.config()
> config.members[0].tags = [{"normal" : "A"}]
> config.members[1].tags = [{"normal" : "B"}]
> config.members[2].tags = [{"normal" : "C"}]
> config.members[3].tags = [{"normal" : "D"}]

> config.settings.getLastErrorModes = [{"visibleMajority" : {"normal" : 3}}]
> rs.reconfig(config)

db.products.insertOne(
    { "_id": 10, "item": "envelopes", "qty": 100, type: "Self-Sealing" },
    { writeConcern: { "w" : "visibleMajority", wtimeout : 1000 } }
);
```

## Sending Reads to Secondaries

- **Consistency Considerations**

    Applications that require below consistencies should not read from secondaries

    - strong consistency
    - read-your-own-write consistency

- **Load Considerations**

    Consider **sharding** instead of relying on the secondaries for load balancing

- **Reasons to Read from Secondaries**

    When primary goes down, use `primaryPreferred` to read from secondaries

    When application needs to get low-latency reads, use `nearest` to read from lowest-latency member, or use **sharding**

    To always read from secondaries, use `secondaryPreferred`

    Have different read requests (with dozens of indexes) for offline processing, **directly connect to a specific secondary** instead of using a replica set connection