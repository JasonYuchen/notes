# Chapter 10 Setting Up a Replica Set

## Introduction to Replication

A replica set is a group of servers with **one primary, the server taking writes**, and multiple secondaries, servers that keep copies of the primaryâ€™s data. If the primary crashes, **the secondaries can elect a new primary** from amongst themselves.

## Setting Up a Replica Set (I)

use **[DNS Seedlist connection format](https://www.mongodb.com/docs/manual/reference/connection-string/#connections-dns-seedlist)** to specify how the applications connect to the replica set.

```shell
$ mongod --bind_ip localhost,192.51.100.1 --replSet mdbDefGuide --dbpath ~/data/rs1 --port 27017 \
    --smallfiles --oplogSize 200
$ mongod --bind_ip localhost,192.51.100.1 --replSet mdbDefGuide --dbpath ~/data/rs2 --port 27018 \
    --smallfiles --oplogSize 200
$ mongod --bind_ip localhost,192.51.100.1 --replSet mdbDefGuide --dbpath ~/data/rs3 --port 27019 \
    --smallfiles --oplogSizs 200
```

## Networking Considerations

Every member of a set must be able to make connections to every other member of the set (including itself)

## Security Considerations

- enable authorization control with an authentication mechanism
- encrypt data on disk
- encrypt data on communication among replica set members
- encrypt data on communication between the set and clients

## Setting Up a Replica Set (II)

**propagate the replica configuration** to all members within a replica set

```json
$ mongo --port 27017

> rsconf = {
    _id: "mdbDefGuide",
    members: [
      {_id: 0, host: "localhost:27017"},
      {_id: 1, host: "localhost:27018"},
      {_id: 2, host: "localhost:27019"} 
    ]
  }
> rs.initiate(rsconf)
{ "ok" : 1, "operationTime" : Timestamp(1501186502, 1) }

// mongdb@27017 will propagate the confiugration to other 2 members

> rs.status()
{
    "set" : "mdbDefGuide",
    "date" : ISODate("2017-07-27T20:23:31.457Z"),
    "myState" : 1,
    "term" : NumberLong(1),
    "heartbeatIntervalMillis" : NumberLong(2000),
    "optimes" : {
        "lastCommittedOpTime" : {
        "ts" : Timestamp(1501187006, 1),
            "t" : NumberLong(1)
        },
        "appliedOpTime" : {
            "ts" : Timestamp(1501187006, 1),
            "t" : NumberLong(1)
        },
        "durableOpTime" : {
            "ts" : Timestamp(1501187006, 1),
            "t" : NumberLong(1)
        }
    },
    "members" : [
        {
            "_id" : 0,
            "name" : "localhost:27017",
            "health" : 1,
            "state" : 1,
            "stateStr" : "PRIMARY",
            "uptime" : 688,
            "optime" : {
                "ts" : Timestamp(1501187006, 1),
                "t" : NumberLong(1)
            },
            "optimeDate" : ISODate("2017-07-27T20:23:26Z"),
            "electionTime" : Timestamp(1501186514, 1),
            "electionDate" : ISODate("2017-07-27T20:15:14Z"),
            "configVersion" : 1,
            "self" : true
        },
        {
            "_id" : 1,
            "name" : "localhost:27018",
            "health" : 1,
            "state" : 2,
            "stateStr" : "SECONDARY",
            "uptime" : 508,
            "optime" : {
                "ts" : Timestamp(1501187006, 1),
                "t" : NumberLong(1)
            },
            "optimeDurable" : {
                "ts" : Timestamp(1501187006, 1),
                "t" : NumberLong(1)
            },
            "optimeDate" : ISODate("2017-07-27T20:23:26Z"),
            "optimeDurableDate" : ISODate("2017-07-27T20:23:26Z"),
            "lastHeartbeat" : ISODate("2017-07-27T20:23:30.818Z"),
            "lastHeartbeatRecv" : ISODate("2017-07-27T20:23:30.113Z"),
            "pingMs" : NumberLong(0),
            "syncingTo" : "localhost:27017",
            "configVersion" : 1
        },
        {
            "_id" : 2,
            "name" : "localhost:27019",
            "health" : 1,
            "state" : 2,
            "stateStr" : "SECONDARY",
            "uptime" : 508,
            "optime" : {
                "ts" : Timestamp(1501187006, 1),
                "t" : NumberLong(1)
            },
            "optimeDurable" : {
                "ts" : Timestamp(1501187006, 1),
                "t" : NumberLong(1)
            },
            "optimeDate" : ISODate("2017-07-27T20:23:26Z"),
            "optimeDurableDate" : ISODate("2017-07-27T20:23:26Z"),
            "lastHeartbeat" : ISODate("2017-07-27T20:23:30.818Z"),
            "lastHeartbeatRecv" : ISODate("2017-07-27T20:23:30.113Z"),
            "pingMs" : NumberLong(0),
            "syncingTo" : "localhost:27017",
            "configVersion" : 1
        }
    ],
    "ok" : 1,
    "operationTime" : Timestamp(1501187006, 1)
}
```

## Observing Replication

use `db.isMaster()` to check the detail of the replica set

```json
> db.isMaster()
{
    "hosts" : [
        "localhost:27017",
        "localhost:27018",
        "localhost:27019"
    ],
    "setName" : "mdbDefGuide",
    "setVersion" : 1,
    "ismaster" : true,
    "secondary" : false,
    "primary" : "localhost:27017",
    "me" : "localhost:27017",
    "electionId" : ObjectId("7fffffff0000000000000004"),
    "lastWrite" : {
        "opTime" : {
            "ts" : Timestamp(1501198208, 1),
            "t" : NumberLong(4)
        },
        "lastWriteDate" : ISODate("2017-07-27T23:30:08Z")
    },
    "maxBsonObjectSize" : 16777216,
    "maxMessageSizeBytes" : 48000000,
    "maxWriteBatchSize" : 1000,
    "localTime" : ISODate("2017-07-27T23:30:08.722Z"),
    "maxWireVersion" : 6,
    "minWireVersion" : 0,
    "readOnly" : false,
    "compression" : [
        "snappy"
    ],
    "ok" : 1,
    "operationTime" : Timestamp(1501198208, 1)
}
```

attempt to do a read on secondary will fail by default (use `secondaryConn.setSlaveOk()` to allow **stale read**)

```json
> secondaryDB.coll.find()
Error: error: {
    "operationTime" : Timestamp(1501200089, 1),
    "ok" : 0,
    "errmsg" : "not master and slaveOk=false",
    "code" : 13435,
    "codeName" : "NotMasterNoSlaveOk"
}

> secondaryConn.setSlaveOk()
> secondaryDB.coll.find()
{ "_id" : ObjectId("597a750696fd35621b4b85db"), "count" : 0 }
{ "_id" : ObjectId("597a750696fd35621b4b85dc"), "count" : 1 }
...
```

attempt to do a write on secondary will always fail

```json
> secondaryDB.coll.insert({"count" : 1001})
WriteResult({ "writeError" : { "code" : 10107, "errmsg" : "not master" } })
```

**automatic failover when primary goes down**

```
> db.adminCommand({"shutdown" : 1})

2017-07-27T20:10:50.612-0400 E QUERY    [thread1] Error: error doing query: 
 failed: network error while attempting to run command 'shutdown' on host 
 '127.0.0.1:27017'  :
DB.prototype.runCommand@src/mongo/shell/db.js:163:1
DB.prototype.adminCommand@src/mongo/shell/db.js:179:16
@(shell):1:1
2017-07-27T20:10:50.614-0400 I NETWORK  [thread1] trying reconnect to 
 127.0.0.1:27017 (127.0.0.1) failed
2017-07-27T20:10:50.615-0400 I NETWORK  [thread1] reconnect 
 127.0.0.1:27017 (127.0.0.1) ok
MongoDB Enterprise mdbDefGuide:SECONDARY> 
2017-07-27T20:10:56.051-0400 I NETWORK  [thread1] trying reconnect to 
 127.0.0.1:27017 (127.0.0.1) failed
2017-07-27T20:10:56.051-0400 W NETWORK  [thread1] Failed to connect to 
 127.0.0.1:27017, in(checking socket for error after poll), reason: 
 Connection refused
2017-07-27T20:10:56.051-0400 I NETWORK  [thread1] reconnect 
 127.0.0.1:27017 (127.0.0.1) failed failed 
MongoDB Enterprise > 
MongoDB Enterprise > secondaryConn.isMaster()

> secondaryDB.isMaster()
{
    "hosts" : [
        "localhost:27017",
        "localhost:27018",
        "localhost:27019"
    ],
    "setName" : "mdbDefGuide",
    "setVersion" : 1,
    "ismaster" : true,
    "secondary" : false,
    "primary" : "localhost:27018",
...
```

## Changing Your Replica Set Configuration

```
> rs.add("localhost:27020")

> rs.remove("localhost:27017")
{ "ok" : 1, "operationTime" : Timestamp(1501202441, 2) }

> var config = rs.config()
> config.members[0].host = "localhost:27017
> rs.reconfig(config)
```

## How to Design a Set

common configurations that are recommended:

- A **majority of the set in one data center** if you have a **primary data center**
- An equal number of servers in each data center with a **tie-breaking server in a third data center** if your **data centers are equal in preference**

***MongoDB uses RAFT consensus protocol***

## Member Configuration Options

- **Priority**

    how strongly this member "wants" to become primary, if the replica set is healthy, a primary with lower priority will **transfer leadership to a secondary with higher priority**

- **Hidden Members**

    **clients do not route requests to hidden members**, and hidden members are not preferred as replication sources (like less powerful or backup servers)

    ```
    > var config = rs.config()
    > config.members[2].hidden = true
    0
    > config.members[2].priority = 0
    0
    > rs.reconfig(config)


    > rs.isMaster()
    {
        ...
        "hosts" : [
            "server-1:27107",
            "server-2:27017"
        ],
        ...
    }
    ```

- **Election Arbiters**

    **RAFT witness member** that only participates in elections, arbiters hold no data and aren't used by clients

    Arbiters are good at breaking ties in larger clusters with **even** number of nodes and speed up the election, but **use at most one arbiter**, and always **prefer a data node (and an odd cluster) to an arbiter** whenever possible

    ```
    > rs.add({"_id" : 4, "host" : "server-5:27017", "arbiterOnly" : true})
    ```

    *three-member replica sets with a primary-secondary-arbiter, PSA, architecture may cause cache pressure issue*

- **Building Indexes**

    if a secondary is used for **backup data or offline batch jobs only**, use `"buildIndexes": false` to disable indexes (permanent setting)
