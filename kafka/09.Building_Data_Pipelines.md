# Chapter 09. Building Data Pipelines

## Considerations When Building Data Pipelines

- **Timeliness**
  不同的系统对数据的吞吐和模式有不同的要求，例如一些系统希望数据每天批量处理一次，另一些系统希望数据一旦生成就在毫秒内被处理，Kafka可以被视作是一个拥有巨大容量的可扩展、可靠的数据缓存区，可以支持各种时间要求的数据处理，**将producer和consumer完全解耦**

  producer可以以近实时的性能生产数据，而相应的consumer则可以少次批量的处理，或相反，Kafka在producer侧通过**延迟ACK（配合最大在途请求数）达到背压流控**，而consumer侧则是完全**主动的`poll()`方式获取消息并处理，实现应用层的流控**
- **Reliability**
  可靠是绝大多数业务系统的最基本要求，大多数系统**不能容忍单点故障、过长的恢复时间、丢失数据**，从第七、八章的讨论可以看出，Kafka天然支持至少一次语义，并且**当外部系统支持事务模型或拥有UID去重机制时就可以达到恰好一次**
- **High and Varying Throughput**
  现代大规模系统对吞吐量同时也有较高要求，不仅可以**支持高吞吐量，同时可以有效应对峰值吞吐量**，采用Kafka后就可以解耦producer和consumer的速率匹配问题，并且当流量上升时可以按需扩容
- **Data Formats**
  Kafka对数据完全无感知，只会根据topic和partition进行消息的读写，至于消息的格式则完全由应用层来决定
- **Transformations**
  通常构建数据流水线有两种基本模式：
  - Extract-Transform-Load ETL：代表着数据流水线需要负责完成数据的转换处理，优点是流水线直接处理数据避免了下游的处理和二次读写代价，缺点在于转换的过程中如果有一些过滤等操作，则下游就无法再看到完整的上游数据
  - Extract-Load-Transform ELT：代表着数据流水线仅做最基本的格式转换等操作，一些逻辑是的过滤、聚合等等都由下游处理，优点是下游能够访问几乎完全一样的上游数据，缺点在于可能会出现数据的二次读写、处理成本较高等

  Kafka Connect支持**单消息转换Single Message Transformation**，可以根据单条输入消息决定路由的topic、过滤、修改数据等等，而更加复杂的聚合、连接等操作则可以依赖Kafka Stream来实现
- **Security**
  `TODO`
- **Failure Handling**
  当出现故障等情况，Kafka存储了较长时间内的所有消息，因此可以在任意时刻通过数据**重放replay**来进行节点恢复
- **Coupling and Agility**
  通常**希望数据的source和sink解耦**，但往往这两者会被意外的耦合在一起，例如：
  - Ad hoc pipelines：采用Logstash将日志导入ES，采用Flume将日志导入HDFS，采用Oracle GoldenGate从Oracle获取数据导入HDFS，等等诸如此类多个模块之间按需建立特有的数据流水线，往往会导致模块与模块之间的强耦合，锁死子模块的技术体系，难以进行修改和技术升级
  - Loss of metadata：若没有统一管理数据流水线的schema等元数据，同样会导致不同子模块的难以修改和升级
  - Extreme processing：当数据的处理经常变化时，则流水线中不同模块可能都要跟随这个变化，改变处理逻辑，导致维护成本高昂，每个阶段的模块弱能接触到最原始的数据则相对最稳定，这些模块可以根据原始数据做出决策，而不必跟随上游模块的修改而修改

## When to Use Kafka Connect Versus Producer and Consumer

当自己维护的应用程序需要读写Kafka时，可以采用传统的producer/consumer模式，这些Kafka client是内嵌在应用程序中的；而当数据source/sink的代码不完全可控，例如需要从HDFS中读取数据并导入到Kafka时则可以选择针对HDFS的Kafka Connector并提供基本的配置即可

当想要处理的数据source/sink没有现有的Kafka Connector时，就可以选择实现一个应用程序内嵌了producer/consumer，或者是实现Connect API（更推荐）

## Kafka Connect

`TODO`

## Alternatives to Kafka Connect

- Ingest Frameworks for Other Datastores
- GUI-Based ETL Tools
- Stream Processing Frameworks
