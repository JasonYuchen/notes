# Chapter 07. Reliable Data Delivery

## Reliability Guarantees

Kafka提供了如下保证：

- **属于同partition的消息有序**，例如若消息B在消息A后由同一个producer写入同一个partition，则Kafka保证消息A的offset在消息B之前，显然consumer也会先消费消息A，再消费到消息B
- 当消息被**写入到所有in-sync replicas ISRs时就可以认为已提交committed**（但不必等待`fsync()`落盘），producer可以自由选择不等待确认、等待leader partition确认或等待ISRs确认
- 只要**至少一个写入的replica存活，则已提交committed的消息不会丢失**
- **consumer只会读取到已提交的消息**

## Replication

replication是Kafka提供可靠性保证的核心机制，具体见[Kafka Internals - Replication](https://github.com/JasonYuchen/notes/blob/master/kafka/06.Kafka_Internals.md#replication)，其中in-sync replica ISR定义为：

- 与ZooKeeper（此后应该是与KRaft）持有一个活跃的session，即在6秒内发送过心跳（时间可配置）
- 从leader replica上持续读取最新消息的，最后一次读取应在10秒内（时间可配置）
- 最近一次读取最新消息的请求能够体现其与leader replica没有任何滞后，即10秒内数据最新（时间可配置）

当一个replica因不满足以上任意一条时，就会被认为out-sync，此时**若一条消息要求写入所有ISRs才算提交，就不再受到这个稍慢导致out-sync节点的影响**，进入out-sync的replica依然会积极获取消息尝试进入in-sync状态，在某些极**端场合下也可能出现out-sync和in-sync反复ping-pong的情况**，需要调整参数稳定集群

## Broker Configuration

- **Replication Factor**, topic-level
  调整`default.replication.factor`可以在broker级别给自动创建的topics确定副本数量，对于设置为N的情况下可以容忍N-1个broker宕机，但是同样引入了写放大，因此该参数的调整往往需要综合考虑：
  - **可用性 Availability**：越多副本，容忍越多brokers宕机，越高可用性
  - **可靠性 Durability**：越多副本，容忍越多磁盘故障导致的永久数据丢失，越高可靠性
  - **吞吐量 Throughput**：越多副本，占用越多带宽用于副本备份，进而影响到集群的吞吐量和延迟
  - **端到端延迟 End-to-end Latency**：越多副本，由于`ack=all`的延迟受ISRs中最慢的节点影响，因此往往越多副本带来的端到端延迟会更高
  - **成本 Cost**：显然越多副本带来的写放大越严重，磁盘空间占用越多，网络带宽占用越多，成本越高

  另外Kafka高度建议**采用rack-awareness的策略来放置不同的replicas，以应对rack级别的失效**（通常**也可以把机架rack认为可用区availability zones或故障域failure domain**）
- **Unclean Leader Election**, broker-level
  若设置`unclean.leader.election.enable=true`，则允许broker宕机时从**out-sync replicas中选举出leader replica**，这意味着已提交消息的丢失，优点是在没有ISRs的时候可以继续提供服务，**即用一致性/可靠性换取可用性**
- **Minimum In-Sync Replicas**，topic/broker-level
  由于采用`ack=all`是认为写入所有ISRs即消息已提交，而从上述可以看出**ISRs的数量是在变动的**，极端情况下可能只有leader是ISR，为了依然保证有多个节点写入数据才算提交，可以通过配置`min.insync.replicas`来强制要求等待至少该数量的ISRs写入才认为提交，在不足这个数量的ISR时producer写入数据就会收到`NotEnoughReplicasException`
- **Keeping Replicas In Sync**，broker-level
  由于in/out-sync状态会极大的影响Kafka诸多功能，并且其状态判定由前述三条规则决定，从而可以通过配置三条规则中的时间约束来调整，包括`zookeeper.session.timeout.ms`、`replica.lag.time.max.ms`
- **Persisting to Disk**
  Kafka并不依赖每个broker主动通过`fsync()`确保数据落盘，而是依赖多个brokers之间replication（这些**replicas被放置在不同的availability zone上，因此极不可能同时出错**）来保证可用性和可靠性，默认情况下Kafka只在创建新segments时主动落盘，其余依赖操作系统页缓存机制的写回磁盘

  Kafka依然提供了配置`flush.messages`和`flush.ms`用于限制处于尚未`fsync()`的最大消息数量或周期性写入磁盘，确保周期性落盘提高可靠性，但是由于落盘的开销较为昂过，这两个配置都有可能极大影响Kafka的吞吐量和延迟

## Using Producers in a Reliable System

即使将Kafka集群以最可靠的配置运行，若producer没有相应的配置也同样可能会出现丢失消息

- **发送确认 Send Acknowledgments**：`acks=0`可能在面临网络波动时就出现丢数据，`acks=1`可能在leader replica回复已提交后立刻宕机，导致数据未及时复制给replicas的情况下出现丢数据，`acks=all`可以确保在写入数据并且提交后就不会出现丢数据，需要配合`min.insync.replicas`
- **配置重试 Configuring Producer Retries**：producer应在处理类似网络错误等**retriable错误**时执行重试，从而确保数据不会丢失，默认重试次数并没有上限，通过`delivery.timeout.ms`来控制总耗时，注意**重试保证了至少一次at-least once语义**，通过`enable.idempotence=true`来实现去重
- **额外的错误处理 Additional Error Handling**：除了可重试错误以外的错误，需要producer侧根据情况谨慎处理，从而尽可能避免丢失数据

## Using Consumers in a Reliable System

由于Kafka保证只有committed的数据才会提供给consumer，因此consumers所见到的数据一定是一致的，关键在于确保消费时正确追踪所消费的数据，即`commit`，而`commit`是一个开销较大的操作

- **影响可靠性的配置 Properties for Reliable Processing**
  - `group.id`：broker会记录同一个consumer group中消费的位置，从而当一个consumer的partition交给另一个consumer时可以从正确的committed offset开始继续消费
  - `auto.offset.reset`：当没有任何committed offset时的起始行为由该参数确定
  - `enable.auto.commit`：是否由consumer来自动提交offset
  - `auto.commit.interval.ms`：当由consumer来提交offset时，其间隔，间隔小意味着开销大，重复消息少，反之开销小但重复消息多
- **显式提交位置 Explicitly Committing Offsets**：手动提交消费的位置需要考虑多个方面的影响，包括：
  - 每次`poll`结束后都进行提交，类似自动提交，但是当涉及多个工作线程时需要特别谨慎
  - 提交频率的设定是**性能**与**重启后重复消费**的权衡
  - 重平衡时也会涉及消费位置的提交
  - **消费过程中出现业务失败时**，例如写入数据库短暂失败，由于消费位置是accumulative-ACK的机制，因此直接提交后面成功的位置会隐式提交失败的消息，当出现业务偶尔失败时，可以考虑以下两种做法：
    1. 仅提交上一个累积成功的位置，并**调用`pause()`暂时停止接收新消息**，执行重试直到业务成功
    2. 将消费失败的数据写入一个**特殊topic**并继续当前消费的过程，而该特殊topic再由一些consumer订阅处理，**类似其他消息系统中的死信队列dead-letter-queue**
- **带状态的消费者 Consumers with State**
  建议考虑Kafka Streams、Flink等提供aggregation、join、windows等复杂带状态流数据分析框架

## Validating System Reliability

可靠的系统需要通过验证确认，通常包括以下三个环节的验证：

- **验证配置 Validating Configuration**
  Kafka提供了两个特别的工具用于辅助配置验证，`VerifiableProducer`和`VerifiableConsumer`，可以通过生成一系列消息并进行消费，来验证所设置的超时、重试等各种参数是否符合预期，测试还需要考虑一些特别的场景包括：
  - **Leader election**：若主动关停leader，producer和consumer需要多久恢复正常工作？
  - **Controller election**：若主动关停controller，集群需要多久恢复正常工作？
  - **Rolling restart**：若逐个重启brokers，是否会出现消息丢失？
  - **Unclear leader election**：若逐个关停一个partition的所有replicas（逐个进入out-of-sync）再重启一个out-of-sync的replica
- **验证应用 Validating Applications**
  验证应用程序的部分在面对以下场景时是否能符合预期：
  - client丢失到一个broker的连接
  - client与broker之间存在较高延迟
  - 磁盘剩余空间不足
  - 磁盘响应缓慢（brown out）
  - leader选举
  - brokers滚动重启
  - consumers滚动重启
  - producers滚动重启
- **监控可靠性 Monitoring Reliability in Production**
  - producer侧最重要的指标就是**error-rate**和**retry-rate**
  - consumer侧最重要的指标就是**consumer lag**
  - 从producer到consumer的端到端延迟
