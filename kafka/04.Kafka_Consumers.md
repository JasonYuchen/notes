# Chapter 04. Kafka Consumers: Reading Data from Kafka

## Kafka Consumer Concepts

- **Consumer 和 Consumer Groups**
  如同多个producers可以写入同一个topic，多个consumers也可以订阅同一个topic从而允许水平扩容来提升消费吞吐量，当属于同一个consumer group的consumers订阅了某个topic时，**每个consumer实际上会收到topic下partitions互不重叠的子集的数据**，如下图：

  ![4.2](images/4.2.png)

  因此**consumers数量的上限可以认为就是partitions数量**，此时每个consumer恰好收到一个partition的数据，进一步增加的consumer只会处于闲置idle状态而不会收到任何数据，通常consumer会做一些数据库更新、计算等耗时较高的作业，因此Kafka的消费侧扩容通常就是向一个consumer group中增加consumers

  对于希望获得所有数据的下游应用，只需要维护自己的consumer group即可，consumer group可以是每个希望获取完整数据的下游维护一个，相同consumer group内部再根据负载来选择增减consumers

  ![4.5](images/4.5.png)

- **Consumer Groups 和 Partition Rebalance**
  在同一个consumer groups内的consumers相当于共享了所有partitions的数据，因此当添加一个新的consumer时，其会**从其他consumer当前消费到的位置继续消费**，同样减少一个consumers时（主动关闭节点或宕机崩溃）其他consumer也会从相应的位置继续开始消费

  Kafka brokers中的某一个节点会作为consumer group的协调者coordinator（不同group可以由不同的broker作为协调者），consumer通过发送定时心跳给协调者来确认存活，同时**通过心跳来维护group membership和partition ownership的信息**，协调者通过心跳来确认consumer的存货情况以及是否需要重平衡

  这种**重新分配consumer所消费的partitions的过程称为重平衡rebalance**，在consumer groups消费的topics被修改时也可能发生，例如增加了新的partitions，重平衡可以分为两类：

  - **积极重平衡 Eager Rebalances**
    在积极重平衡的过程中，所有consumers立即停止消费，并且放弃他们目前所对应的所有partitions，重新加入consumer group从而获得新的partitions，在这个过程中会**造成整个consumer group一小段不可用的窗口**

    ![4.6](images/4.6.png)

  - **协作重平衡 Cooperative Rebalances**
    协作重平衡往往在一小部分partitions和consumers关系需要修改时发生，也被称为**增量重平衡incremental rebalances**，在这个过程中首先consumer group中所有consumers都会被通知**需要放弃的部分partitions**，正在消费相应partitions的consumers随后就会停止消费该partitions而其他保持不变，随后这些**被放弃的部分partitions再被重新分配给新的consumers**开始消费，这个过程包括了多步变动逐渐达到新的状态，避免了整个group都不可用的窗口

    ![4.7](images/4.7.png)

  - **静态成员 Static Group Membership**
    通常，所有consumer在加入consumer group时会获得一个成员ID并且发生重平衡，在后续**崩溃/离开到再加入时获得一个新的成员ID**并且发生重平衡，而通过`group.instance.id`可以使得一个consumer不受心跳超时限制，**在`session.timeout.ms`有效期内始终使用同一个ID，并且不会触发重平衡**，称为静态成员，静态成员在离开后重新加入不会获得新的ID，并且会**重新获得与离开前相同的partitions**，需要注意的是这可能导致部分partitions上的消息在一段时间内没有consumer能够消费

    **静态成员通常用于consumer带状态的场景**，例如维护了较为复杂的与partitions相关的缓存，重平衡则会破坏缓存导致高昂的重建缓存开销，采用静态成员牺牲了短暂的可用性（短暂的不可用窗口）但是保证了缓存的可用性

    **特别注意**：静态成员下线后直到`session.timeout.ms`超时前依然是consumer group的成员，因此不会重平衡，但是超过`session.timeout.ms`后则依然会触发重平衡，因此该参数的选择较为关键

## Creating a Kafka Consumer

与创建producer的方式完全一致，同样有三个必须参数`bootstrap.servers`，`key.deserializer`和`value.deserializer`，另外consumer还可以提供一个可选但通常会指定的参数`group.id`用于指定该consumer所属的consumer group

## Subscribing to Topics

提供订阅的topic就可以通过`subscribe`来读取消息

另外consumer可以提供**正则表达式作为topic**，此时只要符合该正则表达式的所有topic都会被该consumer订阅，并且consumer需要支持能够处理不同topic的消息

```java
consumer.subscribe(Collections.singletonList("customerCountries"));
consumer.subscribe(Pattern.compile("test.*"));
```

需要特别注意的是当集群中包含大量topic，海量partitions时，采用正则表达式可能会导致大量系统资源被用在交换topic和partition的元数据上，形成性能瓶颈

## The Poll Loop

consumer的核心部分就是一个事件循环，**轮询broker来获取新消息并处理**，基本逻辑如下：

```java
while (true) {
  ConsumerRecords<String, String> records = consumer.poll(timeout);
  for (ConsumerRecord<String, String> record : records) {
    System.out.printf("topic = %s, partition = %d, offset = %d, customer = %s, country = %s\n",
                      record.topic(), record.partition(), record.offset(), record.key(), record.value());
    int updatedCount = 1;
    if (custCountryMap.containsKey(record.value())) {
      updatedCount = custCountryMap.get(record.value()) + 1;
    }
    custCountryMap.put(record.value(), updatedCount);
  }
}
```

consumer必须持续不断的调用`consumer.poll(timeout)`，从而broker才会认为该consumer一直存活，避免出现重平衡，`poll()`实际上完成了非常多的工作：

- 首次调用`poll()`时，会负责确定集群中的broker作为协调者group coordinator
- 加入相应的consumer group，或是作为第一个consumer而成为group leader
- 获得partition的分配信息
- 当触发重平衡时，也会在`poll()`内部完成重平衡过程
- `poll()`必须在小于`max.poll.interval.ms`时间内被调用，否则会被认为consumer已经失活（处理**长耗时的消息时必须注意**这一点）

注意：**每个consumer不应该被多个线程共享，同时一个线程也不应该有多个属于同一group的consumer**，通常的做法就是封装consumer并由`ExecutorService`来在每个线程中运行一个consumer

## Configuring Consumers

除了三个必须提供的配置项以外，大多数参数有合理的默认值，其中以下参数的值会显著影响consumer的性能、可靠性、资源占用：

- `fetch.min.bytes`
  单次获取消息的最小数据量，默认是`1B`，若新消息的数据量小于这个值，则broker会等待更多的消息，暂时不推送给consumer，这种方式可以减少consumer和broker的压力，但会带来更高延迟
- `fetch.max.wait.ms`
  设置了`fetch.min.bytes`后，broker会等待不超过这个配置的时间，避免不活跃的topic下broker为了获得最小数据量而等待过长
- `fetch.max.bytes`
  单次获取消息的最大数据量，默认是`50MB`，用于限制consumer的内存占用
- `max.poll.records`
  单次获取消息的最大条数，应用程序应考虑到单次能批量处理的消息数量来设置
- `max.partition.fetch.bytes`
  单次获取消息每个partition的最大数据量，用于限制`poll()`返回的一批数据中属于单个partition的消息数据量，通常避免修改，仅在希望精细控制时才调整该参数
- `session.timeout.ms`和`heartbeat.interval.ms`
  在没有心跳的情况下broker在超过`session.timeout.ms`时就会认为consumer已经宕机，此时会触发partition重平衡；而`heartbeat.interval.ms`就控制了consumer发送心跳的周期且必须小于前者配置的时间
- `max.poll.interval.ms`
  **心跳是consumer单独线程持续发送的**，证明consumer进程依然存活，但是实际上consumer主循环处理任务有可能会出现死锁等逻辑问题，此时需要类似应用层心跳的机制来保证可用性，`max.poll.interval.ms`定义了consumer在两次`.polL()`之前的最长时间，超过这一限制而未调用`.poll()`同样会被认为consumer不可用，触发重平衡

  注意：该参数只是作为最后退路的备份选择，**不应该依赖该参数**，因为由于消息数量、处理速度等难以预测，过小的值会导致consumer不稳定，默认值是五分钟，远大于`session.timeout.ms`和`heartbeat.interval.ms`
- `default.api.timeout.ms`
- `request.timeout.ms`
  consumer等待broker响应的超时时间，若超时则认为broker不可用，此时consumer会关闭连接并尝试重新链接
- `auto.offset.reset`
  **当consumer无法找到一个有效的partition committed offset时**（第一次读取到该partition，或是consumer宕机时间过长导致原有的committed offset对应的数据已经被垃圾回收从而失效），就会根据该参数有不同的行为
  - `"latest"`：从当前最新的数据开始读取消费
  - `"earliest"`：从目前还存在的最早的有效数据开始读取消费
  - `none`：在没有找到有效partition committed offset时抛出异常
- `enable.auto.commit`
  默认为真，consumer在消费数据时会自动提交消费位置，通常会结合`auto.commit.interval.ms`一起使用，**提交消费位置也是一个有开销的操作**
- `partition.assignment.strategy`
  每个consumer所分配的partition是在一个consumer group中分配的，通常由`PartitionAssignor`类来进行计算和分配，基本策略有：
  - **范围 Range**
    每个consumer都会被分配**每个topic下均分连续的一组partitions**，例如有T1P1，T1P2，T1P3，T2P1，T2P2，T2P3两个topic分别三个partitions，且有C1，C2两个consumers，则C1会获得T1P1-P2，T2P1-P2，而C2会获得T1P3，T2P3
  - **轮次 RoundRobin**
    **所有topics的所有partitions按顺序依次分配**给所有consumer，同样上述的例子，则C1获得T1P1，T1P3，T2P1，而C2会获得T1P2，T2P1，T2P3
  - **粘性 Sticky**
    粘性分配器的目的在于首先保证平衡性，随后出现consumer成员变化时尽可能减少需要重平衡的partitions，**尽可能保留每个consumer所属的partitions**
  - **协作式粘性 Cooperative Sticky**
    分配策略与粘性分配器完全一样，差别在于协作式粘性分配器下，即使某个partition被回收不再分配给原先的consumer后，其依然可以**继续消费未被分配的partition**
  - 另外可以实现自定义的分配器
- `client.id`
  用于识别consumer以及相关的logging、metrics和quotas
- `client.rack`
  默认情况下consumer从leader partition中读取数据，当kafka集群横跨多个数据中心或是可用性区域时，就近从replica partition中获取数据可以有更高的性能，即**rack-awareness**，同样用户也可以提供自定义的`replica.selector.class`
- `group.instance.id`
  用于提供静态成员配置的consumer
- `receive.buffer.bytes`和`send.buffer.bytes`
  TCP连接的收发缓存区大小
- `offsets.retention.minutes`
  注意这是broker的配置选项，当一个consumer group始终有活跃的consumer时，其会负责维护提交的消息位置，而**当一个consumer group为空时，Kafka只会保存group的committed offset不超过该回收时间**，默认是七天

## Commits and Offsets

## Rebalance Listeners

## Consuming Records with Specific Offsets

## But How Do Wer Exit?

## Deserializers

## Standalone Consumer: Why and How to Use a Consumer Without a Group
