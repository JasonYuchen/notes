# Lecture 11. Cache Consistency: Frangipani

阅读材料笔记[Frangipani: A Scalable Distributed File System](Frangipani.md)

## 总体设计 overall design

- 强一致性 strong consistency
- 网络文件系统 network file system
- 底层采用Petal：块存储服务，replicated提升容错，striped+sharded提升性能
- 顶层采用Frangipani：去中心化的decentralized文件服务，采用**cache**提升性能
- 将**复杂度留给了clients端，更好的扩展性scalability**，server实现简单且不会成为系统瓶颈

## 缓存可能导致的问题 challenges

- 节点A创建`/grades`，节点B使用`ls /`是否能看到`/grades` -> **coherence**
- 节点A和节点B并发写入数据到`/ab`，实际数据会怎么样 -> **atomicity**
- 节点A在创建文件时（包含分配`i-node`，初始化，更新目录等操作）宕机，导致操作完成了一半，如何进行恢复 -> **crash recovery**

## 缓存一致性 cache coherence

采用**缓存一致性协议来保证线性一致性linearizability的同时提升性能**，缓存一致性协议也被广泛使用在多核CPU、分布式共享内存、文件服务器等

采用**分布式锁服务lock service（依赖于Raft/Paxos等共识算法）**进行缓存协调来保证一致性：

- **读锁read lock**：允许一个server读取并缓存相应的文件数据，当持有读锁的server被要求释放锁时，server也要同时使对应的缓存失效以避免从缓存中读取了过时的数据（已经被其他持有写锁的server修改）
- **写锁write lock**：允许一个server读写并缓存相应的文件数据，当持有写锁的server被要求释放锁，或是被要求降级为读锁时，server也要确保将缓存的脏数据写回到Petal，如果是降级为读锁，则缓存可以不被失效

例如节点A修改了文件z，节点B读取了文件z，则流程如下：

```text
  Node A            Lock Service            Node B
  read z
    ------request(z)----->
                  owner(z) = Node A
    <------grant(z)-------
  read & cache z
    from Petal
  modify z
    locally
                                            read z
                          <-----request(z)----
    <-----revoke(z)-------
  write back z
    to Petal
    ------release(z)----->
                  owner(z) = Node B
                          -------grant(z)---->
                                            read & cache z
                                              from Petal
```

由分布式锁确保了对文件的读写始终都是在最新的数据上进行的，即强一致性保证strong consistency guarantee

## 原子性 atomicity

Frangipani基于**互斥的写锁**简单实现了文件系统操作的**事务性transactional**：

- 在修改文件数据时，会首先获取所有相关数据的锁，**修改时始终持有锁**
- 在修改完成时才会释放锁，即使**收到`revoke`也会将当前操作执行到结束**
  如果是节点宕机无法`revoke`，此时会被锁服务监测到，并启动恢复recovery

## 故障恢复 recovery

Frangipani采用**预写式日志write-ahead log, WAL进行故障恢复**，在所有操作执行写入数据时，首先将操作**日志写入到Petal**中，在恢复时就可以通过WAL重新进行操作，有如下特点：

- Frangipani**每个节点都拥有私有的一个WAL**，从而避免了全局WAL成为性能瓶颈，一种去中心化decentralized设计，但同样会导致log数量增多
- WAL被存储在Petal上，作为共享存储，任意Frangipani都可以访问其他节点的log
- Frangipani一开始将log写在本地，当被`revoke`时就会将log写入Petal，随后将脏数据写回Petal，最后release锁

当一个持有锁的Frangipani节点宕机时，直到client告知或是有其他Frangipani节点请求获得该锁时，锁服务就会检测到节点宕机，并启动恢复程序，具体如下：

1. Node B申请获得锁，锁服务通知Node A释放锁
2. **锁服务的`revoke`超时，告知Node B启动恢复程序**，并告知Node A的log
3. Node B从Petal上读取Node A的log，将记录的操作回放（基于**版本号version number**，只有log记录的version比Petal上metadata block的version要新的log才会被真正执行）并且**Node B不需要获取锁**
4. Node B处理结束，通知锁服务释放锁

```text
S1: delete(z/text)                  crash
S2:                 create(z/text)
S3:                                        recover S1

S3 replay the log of S1, found that the ver. of delete(z/text) is 1
S3 scanned the metadata and the current version by S2's create(z/text) is 2
S3 skipped delete(z/text)
```

与Linux的文件系统提供的保证一样，Frangipani所有操作的WAL仅记录文件元数据的变化，**内容的修改是通过另外的路径执行的**，从而如果仅修改了元数据即宕机，内容数据会丢失（**Liunx系统提供的`fsync`**专门用于同步等待内容修改完成）

## 网络分区 network partition

Frangipani采用**锁带有租约lease的方式应对网络分区**

当出现网络分区时，原先拥有锁的Node A可能被隔离无法与锁服务通信，而锁服务也已经判断Node A宕机（租约过期）并将锁交给Node B开始recovery，此时的Node A无法连接锁服务且自身租约过期，会立即放弃所有修改，并且不再访问Petal

## 局限性 limitations

- 仅仅作为一个workstation的文件服务器较为合适，所有clients主要都是操作属于自己的文件，冲突小，数据少
- 模仿Linux文件系统的接口实用性有限，对于数据库高并发、海量数据等场景，cache和read/write锁的设计极大限制了性能
- Frangipani底层的Petal是共享存储，因此所有节点互相必须可信
- Frangipani/Petal的分离设计较为古怪，各自都有log，更多的网络通信
