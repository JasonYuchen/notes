# [VLDB 2019] LSM-based Storage Techniques: A Survey

## 1 Introduction

Log-Structured Merge-tree, LSM-tree已经在绝大多数现代NoSQL系统中作为底层存储引擎所使用，例如BigTable、Dynamo、HBase、Cassandra、LevelDB、RocksDB等，并且在实时数据处理、图数据处理、流数据处理、OLTP等领域被广泛使用

这篇论文作为一篇综述，主要是总结了LSM-tree本身的特性、学术界和工业界基于LSM-tree的各种修改和提升，以及这些修改本身的权衡与利弊

## 2 LSM-tree Basics

### 2.1 History of LSM-trees

索引通常有两种更新策略，**原地更新 in-place updates**或者是**非原地更新 out-of-place updates**

- **in-place**：典型结构就是B+树，在更新时直接覆盖原先的数据，这种设计往往**对读请求更为友好**，读到的数据一定是最近更新的新数据，而**对写请求就不友好**，写数据时需要寻找到修改的数据点，这个过程引入了**随机I/O**，并且在更新和删除的过程中会导致碎片化，降低了空间利用率
- **out-of-place**：典型结构就是LSM树，所有更新都会被暂存在新的位置而不是直接覆盖旧的数据，从而写数据的过程是**顺序I/O**，**对写请求更为友好**，并且过程中并不会直接覆盖旧的数据因此也有利于简化故障恢复recovery的过程，但是**对读请求就不友好**，读到最新数据的过程更为冗长不像in-place可以直接读到最新数据，另一方面由于写入的数据可能存储在多个位置造成空间浪费，因此往往需要有后台清理服务持续**压紧数据compaction**

在LSM树之前的log-structured storage面临一个严重的问题：所有数据都追加到日志的末尾导致了查询性能低下，因为相关的记录互相分散存储在日志的不同位置不利于快速查询最新结果，同时这也导致了空间浪费

LSM树通过设计了一个**合并过程merge process**来解决上述问题，其特点与发展如下：

- 原始LSM树包含了一系列组成部分`C0, C1, ... Ck`，每一个部分都是B+树，`C0`存储中内存中并服务写请求，其余所有`C1, ... Ck`均存放在磁盘上
- 当任意`Ci`满时就会触发滚动合并过程，将`Ci`的一部分叶节点移动合并给`Ci+1`，也被称为**leveling merge policy**（由于实现的复杂性，这种合并设计并未被广泛使用）
- 在稳定的工作负载下，当level的数量固定时，**写性能在所有相邻的组成部分其大小比例相等时`Ti=|Ci+1|/|Ci|`达到最佳**（这影响了所有后续LSM树的设计与实现）
- 与原始LSM树同时期有另一种合并策略**stepped-merge policy**，其设计为一个LSM树由多个层构成，每一层`L`都由`T`个组成部分，当该层`L`充满时，响应的所有`T`个组成部分一起被合并为单个组成部分并作为`L+1`层的一个组成部分，也被称为**tiering merge policy**（被广泛使用在现在的LSM树实现中）

![3](images/LSM_survey3.png)

### 2.2 Today's LSM-trees

1. **Basic Structure**
   现在的LSM树设计基本上沿袭了原始LSM树的核心设计，并且所有磁盘上的**组成部分都是不可变的immutability**，当需要合并时直接读取被合并的组成部分并生成新的组成部分，从而并发控制和故障恢复更为简单

   一个LSM树的组成部分可以采用任何索引结构，现在的LSM树通常会采用利于并发的数据结构例如**跳表skip-list或是B+树作为内存中的组成部分**，而采用**B+树或有sorted-string table, SSTables作为磁盘上的组成部分**，一个SSTable包含一系列数据块以及一个索引块，数据块存储根据key排序后的key-value对，而索引块存储所有数据块的key范围，[具体见此](https://github.com/JasonYuchen/notes/blob/master/ddia/03.Storage_and_Retrieval.md#2-sorted-string-table-sstable%E5%92%8Clog-structured-merge-trees-lsm-trees)

   一个点查询请求需要依次搜索多个LSM树的组成部分来**确定数据的最新值即reconciliation**，通常从内存的组成部分开始逐个遍历，**一旦找到key就是最新的值**，但是没有找到就需要一直遍历直到所有组成部分才能最终确定这个key是否存在以及对应的value的最新值，对于**范围查询则是同时遍历所有组成部分**并且将满足范围的结果加入到结果集中

   随着正常运行，磁盘上的组成部分会越来越多，此时需要通过前文描述的合并过程进行合并组成部分、仅保留key的较新值、剔除被标记删除的值，如上图所示，最优的情况就是每一层之间的尺寸比例为`T`，因此在**leveling merge policy下每一层只有单个组成部分**，并且这个组成部分的尺寸是上一层的`T`倍大小，当尺寸达到上一层的`T`倍时该层才会被放入下一层；而在**tiering merge policy下每一层都有至多`T`个组成部分**，当任意层达到`T`时就会全部被合并为一个组成部分并放在下一层中

   对于leveling merge policy而言，由于每一层只有一个组成部分，因此**对读请求相对友好，每一层只需查询一个组成部分**；对于tiering merge policy而言，由于每一层可以有更多的组成部分，因此**对写请求更加友好，多个组成部分减少了合并的频率**

2. **Well-Known Optimizations**
   在现在大多数LSM树实现中都采用了以下两大优化措施：
   - **布隆过滤器 Bloom Filter**
     当插入新key时，key被散列多次映射到位向量的多个不同位置，这些位置被置1；当判断一个key是否存在时就通过相同的方式判断位向量的多个不同位置是否为1，只要有1个位置是0就说明不存在，如果所有位置都是1就说明**可能存在**，需要考虑到散列冲突的情况，因此布隆过滤器是一个概率查询结构，可能存在假阳性，但一定不存在假阴性

     **对磁盘上的LSM树组成部分构建内存中的布隆过滤器**，从而当一定不存在时就可以避免读取磁盘数据，显著提升查询速度，当可能存在时才读取磁盘上的B+树索引进而精确判定组成部分中是否有这个key

     另一种措施是**只对组成部分的B+树索引叶结点构建布隆过滤器**，此时依然需要首先通过B+树索引非叶节点部分（通常这种方式往往认为非叶节点部分足够小完全可以放置在内存中）来确定叶节点，随后先读取布隆过滤器来确定叶节点中是否有可能存在需要的数据而不是真正去读取叶节点数据

     布隆过滤器的假阳性概率为 $(1-e^{-kn/m})^k$，其中 $k$ 为散列函数的数量， $n$ 为key的数量， $m$ 为位向量的长度，假阳性率最低时 $k=(m/n)ln2$ ，**实践中通常直接采用`10 bits/key`从而获得约1%的假阳性率**
   - **分区 Partitioning**
     单个磁盘组成部分较大时会有诸多性能劣化的可能性，采用**范围分区**的方式将组成部分分割为多个较小的分区，为了便于理解**每个分区称为一个SSTable**，将一个较大的组成部分分为多个小的SSTable后，**有利于合并操作时粒度更细**，即单次处理较小的数据量、产生较小的中间结果、处理时间较短，另一方面也**有利于分割处理数据的范围减少重叠**，只需要合并key范围存在重叠的SSTable

     **分区的优化手段可以与合并策略组合使用**，例如leveling + partitioning或tiering + partitioning如下图，实际实现中例如LevelDB和RocksDB完全实现了leveling + partitioning

     图4中可以看出level 0的组成部分并没有分区，这些内存组成部分是直接刷写到磁盘上的，当需要将SSTable从`L`层合并到`L+1`层时所有`L+1`层中与该SSTable存在key范围重叠的SSTables一起参与合并，如图中的`0-15`和`16-32`需要与`0-30`合并，并且合并后原先的`0-30`就会被垃圾回收，**由于触发合并时任意一个`L`层的SSTable都可以被选择，因此可以有不同的选择算法**，LevelDB采用简单的round-robin策略来减小总的写入成本

     ![4](images/LSM_survey4.png)

     tiering merge policy同样可以使用分区，但是问题在于**tiering下一层可以有多个组成部分并且其key范围存在重叠**，从而当分区后可能导致多个存在重叠范围的SSTables，而leveling merge polcy中每一层只有一个组成部分可以简单分区成互不重叠的SSTables，此时如上图中的设计可以引入**垂直分组vertical grouping**或**水平分组horizontal grouping**对SSTables进行管理以确保正确的合并

     **垂直分组**中每个组内的SSTables都存在重叠，而组之间则不存在重叠，从而**触发合并时以组为单位**，合并某一组内的所有SSTables来产生下一层新的SSTable并插入对应的组中

     **水平分组**内每个SSTables之间都不存在重叠，因此一个组成部分进行逻辑分区后就可以直接作为一个组，每一层的多个组成部分作为多个组，只有一个作为**活跃组**并接收上一层合并产生的新SSTables，当合并时需要选择所有组的key重叠部分进行合并，产生的新SSTables就加入下一层的活跃组

3. **Concurrency Control and Recovery**
   LSM树的并发支持通常采用**锁机制locking scheme**或是**多版本机制multi-version scheme**来实现，由于LSM树本身会保存key的多个版本并且其合并操作会丢弃过时的数据，因此很自然的可以支持多版本并发控制，但是LSM树特有的**合并操作会对元数据做出修改**，因此必须被同步，通常可以对**每个组成部分维护一个引用计数**，在访问LSM树前，首先获得**当前所有活跃组成部分的快照**，并增加其引用计数从而保证使用中的组成部分不会因为合并而被垃圾回收

   由于所有写入首先都追加到内存中，使用WAL就可以保证写入数据的持久可靠，**通常的LSM树会采用[no-steal](https://github.com/JasonYuchen/notes/blob/master/cmu15.445/20.Logging.md#%E7%BC%93%E5%AD%98%E6%B1%A0%E7%AD%96%E7%95%A5-buffer-pool-policies)的缓存管理策略**，内存中的组成部分只有在所有活跃的写入事务结束时才会被刷写到磁盘上，在**恢复时因为no-steal的策略从而只需要redo所有成功的事务即可，不需要undo未完成的事务**，因为这些事务并没有刷写到磁盘上；另外需要确保活跃组成部分的列表也能够被恢复，在LevelDB和RocksDB中这通过**额外维护一个元数据日志metadata log来记录所有结构上的修改**，例如SSTables的增减
4. Cost Analysis

### 2.3 Cost Analysis
