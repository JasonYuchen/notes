# Chapter 5. Exactly-Once and Side Effects

## 为什么要恰好一次 Why Exactly Once Matters

在流数据无法做到精确计算结果时即没有恰好一次时，往往会采用**Lambda架构：双写数据**，批处理计算出精确结果但延迟高，流数据计算出粗略结果但延迟低，这种架构带来了更多的麻烦：

- 不准确 Inaccuracy
- 不一致 Inconsistency
- 复杂 Complexity
- 不可预测 Unpredictability
- 延迟 Latency

本章介绍了Google Cloud Dataflow实现恰好一次的策略，基于Google Cloud Pub/Sub，后者是一种多发布多订阅消息系统，并且**不保证传递顺序、不保证同一个任务传递给同一个接收者**

## 精确性和完整性 Accuracy vs. Completeness

通常在处理数据时，我们总是希望每一条数据既不被丢弃也不会重复，即**恰好一次exactly-once**，在[流数据处理系统的流程中](https://github.com/JasonYuchen/notes/blob/master/streamingsystems/02.What_Where_When_How.md#when-allowed-lateness)，一个窗口内（包括early/on-time/late）的数据应该得到恰好一次处理，而超过了最大允许延迟后的数据就被显式丢弃

- **副作用 Side Effects**
  在灵活的流处理系统实现中，往往允许用户自定义一些处理逻辑custom transformation，当处理数据时就会调用自定义处理逻辑，而调用次数并不一定是恰好一次

  通常为了容忍worker宕机，每一条数据都会**至少处理一次at-least-once**，此时如果自定义逻辑包含**非幂等nonidempotent**的部分，就会产生副作用
  
  实践中这种非幂等的副作用往往是必然发生的，流数据系统末端最终需要**将结果输出给外部系统，而这种结果输出不是幂等的**，因此外部系统在首次收到流数据系统的输出结果时往往需要通过一些业务上的知识进行结果的去重处理，即幂等化

- **问题定义 Problem Definition**
  **端到端恰好一次end-to-end exactly once**保证由以下三个部分分别恰好一次处理来实现：
  - **Shuffle**：在流数据处理系统中，数据往往会根据需要往返多个不同的分布式节点，例如GroupByKey操作，这种数据根据需要重组分区归类的过程就是**shuffle**，显然在shuffle中需要保证每个数据只会被shuffle一次
  - **Sources**：数据源需要保证产生的数据也是不会重复的，每一条数据都是独一无二的
  - **Sink**：数据终点同样也需要保证每个数据的处理结果只会产生一次

## shuffle恰好一次 Ensuring Exactly Once in Shuffle

分布式系统中通常使用Remote Procedure Call, RPC进行通信，包括传输数据，而RPC有可能因为各种原因失败，为了避免丢失RPC需要有重试的机制来确保可靠传递数据，实现**至少一次at-least-once成功传递数据**

引入重试之后，同样有可能因为各种原因导致表面上失败的RPC实际上成功传递了数据，例如超时引起了重试而前序的RPC实际上成功了，此时相当于传递了多份相同的数据，因此RPC需要有去重的机制来确保数据只有一份，实现**至多一次at-most-once成功传递数据**，这种去重机制实际上非常简单，只需要**每个数据记录都带有一个唯一的标签unique identifer**，当收到的数据标签此前已经存在时（例如构建一个KV存储，K就是UID，V就是具体数据，从而对相同的K执行任意次`set(K, V)`结果是相同的，即**采用幂等操作将非幂等操作幂等化**），就可以直接丢弃这份重复数据

引入**重试upstream backup**和**去重deduplication**后，shuffle就可以做到恰好一次

## 确定性 Addressing Determinism

引入重试和去重尚未解决用户自定义逻辑的不确定性带来的副作用，并且在实践中有些场合需要包含不确定性（例如需要随机处理）的自定义逻辑，而另外也有可能是用户没有意识到自定义逻辑隐含了不确定性

通过使用**快照checkpointing**可以使得不确定的处理变为**等效确定effectively deterministic**，每一个transform（包括用户的custom transformation）后的结果都在**发往下一个阶段前先与UID一起快照保存**在持久化存储中，随后当**重试时首先检查UID是否已经有快照保存的结果**，若有则直接返回先前的结果，本质原理和KV存储的幂等性使非幂等操作幂等化相同

## 性能 Performance

为了达成恰好一次处理，每个数据都会携带相应的UID，并且当处理结束时输出结果也会被快照保存到持久存储上，而整个过程会引入额外的开销，因此需要尽可能优化I/O

### 图优化 Graph Optimization

流数据处理系统往往分为多个阶段/节点处理数据，可以从处理图（类似Spark lineage的DAG）应用两种优化：

- **融合处理 Fusion**
  将逻辑上的多个步骤合并成单个执行阶段，从而显著减小I/O次数并简化执行流

  ![5.3](images/5.3.png)

- **合并 Combiner Lifting**
  优化满足**结合律associative**和**交换律commutative**的聚合操作（例如`Count`和`Sum`），在局部先执行部分聚合，随后再将数据发送给主要的聚合算子，即**部分聚合提前执行combiner lifting**，例如下游节点是`Sum`则上游每个节点都先执行局部`Sum`在发送给下游而不是每个数据都发送给下游

  ![5.4](images/5.4.png)

### 布隆过滤器 Bloom Filters

对于一个稳定运行状况良好的流处理系统来说，数据重传导致重复的可能性非常低，而每一条数据都必须通过UID来确认是独一无二的，因此能够**高效查找UID是否重复**是提升流数据系统正常情况下的吞吐量的核心

**采用布隆过滤器进行UID的查找**，当一条数据到来时首先进行布隆过滤器查找，**假如不存在则可以确保不存在**进而避免更昂贵的从持久化存储中查找，并加入到布隆过滤器中，**假如存在则有一定概率概率存在**，此时再去持久化存储中查找，如前所述正常运行的情况下重复的概率非常低，因此最后需要去持久化存储中查找的概率极低

随着数据处理的进行，**布隆过滤器中数据越多则假阳性率越高**（布隆过滤器认为存在但实际不存在），此时需要重建布隆过滤器，通常可以**对布隆过滤器按时间分组**，例如每隔10分组一个新的布隆过滤器，系统对每条数据进行对应的布隆过滤器查找，并在一段时间后**垃圾回收过时的布隆过滤器**

### 垃圾回收 Garbage Collection

每个节点都维护了UID的持久化存储，而对于过时的数据就可以及时垃圾回收

- 方式一：对每条数据都带上**严格单调递增的序列号**（例如UID采用单调递增的序列号），则节点可以记录**当前未见到的最小序列号**，在此序列号之前的数据已经见到过了则可以安全垃圾回收，并且后续有任何小于此序列号的数据都一定时重复的，不需要查询持久化存储而直接丢弃
- 方式二：在前述使用布隆过滤器时对每条数据都加上了抵达节点的时间戳（处理时间processing timestamp），则此时节点就可以**通过该抵达时间戳计算出一个垃圾回收水印，从而对于水印时间之前的UID持久化存储就可以直接垃圾回收**，采用处理时间水印的额外好处是可以[观测到系统的处理延迟](https://github.com/JasonYuchen/notes/blob/master/streamingsystems/03.Watermarks.md#%E5%A4%84%E7%90%86%E6%97%B6%E9%97%B4%E6%B0%B4%E5%8D%B0-processing-time-watermarks)

在一些情况下（例如**网络残留network remnants**使得一些数据在网络中卡了任意长的时间而未被丢弃，随后又突然出现在系统中）可能出现数据延迟到水印后才抵达系统，但是当下的设计中只有当数据抵达了系统才会继续处理从而水印才有可能继续前进，**只要当水印继续前进后出现的过旧数据，显然已经被处理过了因此可以简单丢弃**

## source恰好一次 Exactly Once in Sources

对于文件数据源来说，数据是静态且固定顺序的，因此**每次读取都是确定性的且没有任何副作用**，例如Apache Kafka也提供类似的保证，每一个Kafka topic下分成固定的分区partitions且每个分区内的顺序是确定性的，因此从Kafka同一个partition中顺序读取就和从文件读取一样确定性

另外例如Google Cloud Pub/Sub则是非确定性的数据源，当订阅者处理失败时，Pub/Sub会重新发布这条数据到任意一个订阅者，接收的订阅者以及接受顺序均没有保证，因此**数据源需要提供每条数据的UID**从而允许流数据系统可以根据UID进行去重处理

数据源的恰好一次要求数据源的**读取是确定性的，或者对每一条数据提供UID**

## sink恰好一次 Exactly Once in Sinks

从上述讨论来看，sink作为一个外部系统是无法避免数据重复的可能性的，因此最佳方式是将**sink处理的操作幂等化**，例如set/overwrite一个数据库的值就是幂等的操作

另一方面例如**窗口操作也不是幂等的**，当某个窗口收到`e0, e1, e2`数据并完成计算发出结果后而确认处理完前（假如顺序是`process->output->commit`则是output后而commit前）节点宕机，当重启后重新计算此时额外收到了`e3`，就会出现计算`e0, e1, e2, e3`的数据并发出，此时**前后两次相同窗口的输出结果并不相同，因此并不是去重能解决的问题**，此时通过reshuffle由Dataflow确保在reshuffle前只有一种结果可以穿越shuffle边界

> Dataflow currently guarantees that **only one version of a DoFn’s output** can make it past a shuffle boundary

若在系统中生成UID再传递给sink，则在**生成UID的过程后也要加上reshuffle过程**，利用reshuffle来确保提供给sink的操作是幂等的，同一条数据任意次重试都已经带有不变的UID，否则例如这次记录A进入sink时根据时间戳分配了`UID_1234`，而需要重试重新传递记录A时就有可能给记录A分配任意其他UID，使用reshuffle使得UID分配后就是确定不变的

注意：实际上**只要保证结果的输出是稳定的，例如UID并不随机**而是根据数据摘要而成，而并不一定需要reshuffle

## 案例 Use Cases
